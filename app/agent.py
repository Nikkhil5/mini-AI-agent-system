import os
import json
from typing import List, Dict, Any
from search_tool import serpapi_search
from extractor import fetch_url, extract_text_from_html, extract_text_from_pdf_bytes
from db import save_report
import time
import requests


HUGGINGFACE_API_KEY = os.environ.get("HUGGINGFACE_API_KEY")
HUGGINGFACE_API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"


def create_fallback_summary(sources: List[Dict[str, Any]], query: str) -> Dict[str, Any]:
    return {
        "title": f"Research Report: {query}",
        "summary": f"Found {len(sources)} sources related to '{query}'. This report was generated by basic extraction.",
        "key_points": [
            f"Successfully searched for: {query}",
            f"Retrieved {len(sources)} relevant sources",
            "Content extracted from web pages",
            "Sources include academic and news articles" if len(sources) > 1 else "Single source analyzed",
            "Data compiled from extracted web content"
        ][:5],
        "references": [
            {
                "url": src.get("url", ""),
                "note": (src.get("title", "")[:60] + "...") if len(src.get("title", "")) > 60 else src.get("title", "")
            } for src in sources
        ],
        "raw_prompt_response": "Fallback summary generated without Hugging Face"
    }


def summarize_with_huggingface(sources: List[Dict[str, Any]], query: str) -> Dict[str, Any]:
    if not HUGGINGFACE_API_KEY:
        return create_fallback_summary(sources, query)

    max_chars_per_source = 2000  # Limit to 2000 chars per source for faster API response
    combined_text = "\n\n".join(src.get("content", "")[:max_chars_per_source] for src in sources)

    headers = {
        "Authorization": f"Bearer {HUGGINGFACE_API_KEY}"
    }

    payload = {
        "inputs": combined_text
    }

    try:
        response = requests.post(HUGGINGFACE_API_URL, headers=headers, json=payload, timeout=60)  # 60 sec timeout
        response.raise_for_status()
        data = response.json()

        summary_text = ""
        if isinstance(data, list) and len(data) > 0 and "summary_text" in data[0]:
            summary_text = data[0]["summary_text"]
        else:
            summary_text = "Summary not available from Hugging Face API."

        title = f"Research Report: {query}"
        key_points = [summary_text]
        references = [{"url": src.get("url", ""), "note": src.get("title", "")[:60]} for src in sources]

        return {
            "title": title,
            "summary": summary_text,
            "key_points": key_points,
            "references": references,
            "raw_prompt_response": json.dumps(data)
        }
    except Exception as e:
        print(f"âŒ Hugging Face API call failed: {e}")
        return create_fallback_summary(sources, query)


def run_agent_for_query(query: str) -> Dict[str, Any]:
    start_time = time.time()
    print(f"ðŸ” Starting research for: {query}")

    try:
        print("ðŸŒ Searching with SerpAPI...")
        search_results = serpapi_search(query, num_results=3)
        if not search_results:
            return {"success": False, "error": "No search results found", "report_id": None}
        print(f"âœ… Found {len(search_results)} search results")
    except Exception as e:
        print(f"âŒ Search failed: {e}")
        return {"success": False, "error": f"Search failed: {str(e)}", "report_id": None}

    print("ðŸ“„ Extracting content from sources...")
    sources = []
    skipped_sources = []

    for i, result in enumerate(search_results, 1):
        url = result["link"]
        title = result["title"]
        print(f"  Processing source {i}/{len(search_results)}: {title[:50]}...")

        if not is_valid_url(url):
            skipped_sources.append({"url": url, "title": title, "reason": "Blocked domain"})
            continue

        try:
            content_bytes, content_type, error = fetch_url(url)

            if error:
                skipped_sources.append({"url": url, "title": title, "reason": f"Fetch error: {error}"})
                continue

            if content_type and "pdf" in content_type:
                content_text = extract_text_from_pdf_bytes(content_bytes)
            else:
                content_text = extract_text_from_html(content_bytes, url)

            if content_text and len(content_text.strip()) > 100:
                content_text = content_text[:30000]
                sources.append({"url": url, "title": title, "content": content_text})
                print(f"  âœ… Extracted {len(content_text)} characters")
            else:
                skipped_sources.append({"url": url, "title": title, "reason": "Insufficient content extracted"})
                print(f"  âš ï¸ Skipped: insufficient content")

        except Exception as e:
            skipped_sources.append({"url": url, "title": title, "reason": f"Processing error: {str(e)}"})
            print(f"  âŒ Error: {str(e)}")

    if not sources:
        return {"success": False, "error": "No content could be extracted from any source", "skipped_sources": skipped_sources, "report_id": None}

    print(f"âœ… Successfully extracted content from {len(sources)} sources")

    print("ðŸ¤– Generating summary...")
    try:
        report = summarize_with_huggingface(sources, query)
        print("âœ… Summary generated successfully")
    except Exception as e:
        print(f"âŒ Summarization failed: {e}")
        return {"success": False, "error": f"Summarization failed: {str(e)}", "sources_found": len(sources), "skipped_sources": skipped_sources, "report_id": None}

    print("ðŸ’¾ Saving to database...")
    try:
        report_obj = {
            "query": query,
            "sources": sources,
            "skipped_sources": skipped_sources,
            "report": report,
            "processing_time": round(time.time() - start_time, 2)
        }
        report_id = save_report(query=query, title=report["title"], summary=report["summary"], report_obj=report_obj)
        print(f"âœ… Report saved with ID: {report_id}")
        print(f"â±ï¸ Total processing time: {report_obj['processing_time']}s")

        return {
            "success": True,
            "report_id": report_id,
            "sources_found": len(sources),
            "skipped_sources": skipped_sources,
            "processing_time": report_obj["processing_time"]
        }
    except Exception as e:
        print(f"âŒ Database save failed: {e}")
        return {"success": False, "error": f"Database save failed: {str(e)}", "sources_found": len(sources), "skipped_sources": skipped_sources, "report_id": None}


def is_valid_url(url: str) -> bool:
    blocked_domains = [
        'youtube.com', 'youtu.be', 'twitter.com', 'x.com',
        'facebook.com', 'instagram.com', 'linkedin.com',
        'reddit.com', 'tiktok.com'
    ]

    blocked_extensions = ['.mp4', '.mp3', '.jpg', '.png', '.gif', '.zip', '.exe']

    url_lower = url.lower()

    if any(domain in url_lower for domain in blocked_domains):
        return False

    if any(ext in url_lower for ext in blocked_extensions):
        return False

    return True


if __name__ == "__main__":
    test_query = "latest AI developments 2024"
    print(f"Testing agent with query: {test_query}")
    result = run_agent_for_query(test_query)
    print("\nResult:")
    print(json.dumps(result, indent=2))
